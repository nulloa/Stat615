\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathrsfs,fancyhdr,syntonly,lastpage,hyperref,enumitem,graphicx}

\hypersetup{colorlinks=true,urlcolor=black}

\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}


\begin{document}
\lhead{Homework 2}
\chead{STAT 615 - Advanced Bayesian Methods}
\rhead{Nehemias Ulloa}

\begin{enumerate}

  \item The Horseshoe distribution can be described as follows:
  \begin{align*}
  y_{ig} &\overset{ind}{\sim} N(x_i\theta_g, \sigma^2) \\
  \theta_g &\overset{ind}{\sim} N(0, \lambda_g^2 \tau^2) \\
  \lambda_g^2 &\sim Ca^{+}(0,1) \\
  \tau^2 &\sim Ca^{+}(0,1) \\
  \pi(\sigma) &\propto 1/\sigma
  \end{align*}
  
  %%%%% START THE DIFF PARTS %%%%%
  \begin{enumerate}
  %%%%% First Part %%%%%
  \item 
  The goal for this sampler is to make it in as few steps as possible.
  Here are the steps used:
  \begin{enumerate}
    \item Sample $\theta = (\theta_1, \cdots, \theta_G) \sim p(\theta|\cdots)$
    \begin{enumerate}
      \item For $g=1,\cdots,G$, sample $\theta_g \sim N(\mu_g,\gamma_g^2)$
    \end{enumerate}
    \item Sample $\sigma, \lambda_g^2, \tau$
    \begin{enumerate}
      \item Sample $\sigma^2 \sim IG\bigg(\frac{n}{2}, \frac{1}{2} \sum_{g=1}^{G} \sum_{i=1}^{n_g}(y_{ig} - \theta_g)^2\bigg)$
      \item Sample $\lambda_i^2$ via a Metropolis-Hastings step the posterior from using the standard non-informative prior as its proposal
      \item Sample $\tau^2$ via a Metropolis-Hastings step the posterior from using the standard non-informative prior as its proposal \\
    \end{enumerate}
  \end{enumerate}
  
  More specifically, we'll sample $\theta$ via:
  \begin{align*}
    \theta_g|\cdots &\overset{ind}{\sim} N(\mu_g,\gamma_g^2) \\
    \gamma_g^2  &= \left[ \frac{1}{\lambda_i^2 \tau_i^2} + \frac{n_g}{\sigma^2} \right]^{-1} \\
    \mu_g &= \gamma_g^2 \left[ 0*\lambda_i^{-2} \tau_i^{-2} + \bar{y}_g n_g \sigma^{-2} \right] \\ 
    &= \gamma_g^2 \left[\bar{y}_g n_g \sigma^{-2} \right] \\
  \end{align*}
  
  
  $\sigma$ via:
  \begin{equation*}
    \sigma|\cdots \sim IG\bigg(\frac{n}{2}, \frac{1}{2} \sum_{g=1}^{G} \sum_{i=1}^{n_g}(y_{ig} - \theta_g)^2\bigg)\\
  \end{equation*}
  
  
    $\lambda_i^2$ is sampled via a Metropolis-Hastings step since it's conditional posterior is unknown. For the proposal, we will use the posterior from using the standard non-informative prior($p(\lambda_i^2) \propto 1/\lambda_i^2$):
  \begin{equation*}
    \lambda_i^2|\cdots \sim IG\bigg(\frac{n}{2}, \frac{1}{2\tau^2} \sum_{g=1}^{G} (\theta_g)^2\bigg)\\
  \end{equation*}
  
  Similarly, $\tau^2$ is sampled via a Metropolis-Hastings step since it's conditional posterior is unknown. For the proposal, we will use the posterior from using the standard non-informative prior($p(\tau^2) \propto 1/\tau^2$):
  \begin{equation*}
    \tau^2|\cdots \sim IG\bigg(\frac{n}{2}, \sum_{g=1}^{G} \frac{1}{2\lambda_i^2}(\theta_g)^2\bigg)\\
  \end{equation*}
  
  %%%%% Second Part %%%%%
  \item
  <<Horseshoe Sampler, echo=TRUE, messages=FALSE>>=
  library("dplyr")
  library("Rcpp")
  sourceCpp("homework2.cpp")
  
  
  mcmc_horseshoe <- function(n_reps, y, group, initial_values){
    
    # Set the initial values
    lambda  <- initial_values["lambda"]
    sigma2 <- initial_values["sigma2"]
    tau2   <- initial_values["tau2"]
    theta  <- initial_values["theta"]
    
    # Setup the storage for posterior sample
    G <- length(theta) # number of groups
    keep_theta  <- matrix(rep(NA, n_reps*G), ncol=G, nrow=n_reps)
    keep_lambda <- rep(NA, n_reps)
    keep_sigma2 <- rep(NA, n_reps)
    keep_tau2   <- rep(NA, n_reps)

    
    
    
    
    
    
  }
  @
  
  \end{enumerate}
\end{enumerate}

\end{document}
