\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathrsfs,fancyhdr,syntonly,lastpage,hyperref,enumitem,graphicx}

\hypersetup{colorlinks=true,urlcolor=red}

\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}


\begin{document}
\lhead{Homework 3}
\chead{STAT 615 - Advanced Bayesian Methods}
\rhead{Nehemias Ulloa}

Consider the linear trend model
\begin{align*}
Y_t &= F\theta_t + \nu_t, && \nu_t \stackrel{ind}{\sim}N(0,V) \\
\theta_t &= G\theta_{t-1} + \omega_t,  &&\omega_t \stackrel{ind}{\sim}N_2(0,W) \\
\theta_0 &\sim N(m_0,C_0)
\end{align*}
where
\[
V = \sigma^2, \quad
F = (1,0), \quad
G = \left[ \begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array} \right], \quad 
\mbox{and} \quad
W = \left[ \begin{array}{cc} 0 & 0 \\ 0 & \sigma_\beta^2\end{array} \right].
\]
Let $y=(y_1,\ldots,y_n)$, $\theta = (\theta_0,\ldots,\theta_T)$, and
$\psi = (\sigma^2,\sigma_\beta^2)$.

For simplicity, assume the variances have independent inverse gamma
priors with shape and rate parameters both equal to 1.



\section*{Problem 1}
\begin{align*}
  p(\psi|\theta,y) &\propto p(\sigma^2|\theta,y) p(\sigma^2_{\beta}|\theta,y) \text{via independence}
\end{align*}

so then using the fact that $v_t = y_t - F \theta_t \sim N(0, \sigma^2$,

\begin{align*}
  p(\sigma^2|\theta,y) &\propto p(v_t) p(\sigma^2) \\
                       &\propto (\sigma^2)^{-T/2} exp(\frac{\sum_t(y_t - F \theta_t)^2}{2\sigma^2}) (\sigma^2)^{-1-1} exp(-\frac{1}{\sigma^2}) \\
                       &\propto (\sigma^2)^{-(T/2+1)-1} exp((\frac{1}{\sigma^2})(\frac{\sum_t(y_t - F \theta_t)^2}{2} + 1)) \\
                       &\sim Inv-Gamma(\frac{T}{2} + 1, \frac{\sum_t(y_t - F \theta_t)^2}{2} + 1)
\end{align*}

and similarly for $\sigma^2_{\beta}$, $\sigma^2_{\beta}|\cdots \sim Inv-Gamma(\frac{T}{2} + 1, \frac{\sum_t(\beta_t - \beta_t-1)^2}{2} + 1)$


Here is some basic code setup:
<<message=FALSE>>=
library("dlm")
library("dplyr")
library("reshape2")
library("ggplot2")
OGlake <- read.table("lakeSuperior.dat", col.names = c("year","precipitation_inches"))
lake <- OGlake$precipitation_inches
plot(lake, type ='o', col = "seagreen")


# using mle as starting values
buildFun <- function(x){ dlmModPoly(2, dV = exp(x[1]), dW=c(0,exp(x[2])), m0=c(30,9), C0=matrix(c(100^2,0,0,1^2), 2, 2)) }
fit <- dlmMLE(lake, parm = c(1,1), build = buildFun)
sval <- list(v=exp(fit$par[1]), w=exp(fit$par[2]))
@


\section*{Problem 2}

$S = \left[ \begin{array}{cc} 9.7598368 & -0.17258778 \\ -0.1725878 & 0.03841483 \end{array} \right] $

This $S$ will result in $\approx 41\%$ acceptance in the Metropolis algorithm was obtained using the code below.
<<echo=FALSE, eval=FALSE>>=
mcmc2 <- function(y, niter, sval, Sigma){
  # Summary Stats
  n <- length(y)
  
  # setup storage
  thetakeep <- matrix(data=NA, ncol=2*(n+1), nrow=niter)
  phikeep   <- matrix(data=NA, ncol=2, nrow=niter)
  
  # setup initialization
  prevphi  <- c(sval$v, sval$w)
  
  dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
  
  for(i in 1:niter){
    if(i%%(niter/10) == 0){print(paste(100*i/niter,"% of iterations complete", sep=""))}
    
    # sample phi 
    dphi <- function(phi, y, a, b){
      dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
      tmpPhi <- NULL
      dlmM      <- dlm::dlmModPoly(2, dV = phi[1], dW=c(0,phi[2]), m0=c(10,10), C0=matrix(c(10,0,0,5), 2, 2))
      FiltD     <- dlm::dlmFilter(y, dlmM)
      Rt        <- dlm::dlmSvd2var(FiltD$U.R, FiltD$D.R)
      for(t in 1:length(y)){
        prop_var <- dlmM$FF %*% Rt[[t]] %*% t(dlmM$FF) + dlmM$V
        tmpPhi[t] <- dnorm(y[t], FiltD$f[t], sqrt(prop_var), log=TRUE)
      }
      return(sum(tmpPhi) + log(dinvgamma(phi[1], a, b)) + log(dinvgamma(phi[2], a, b)))
    }
    
    propphi <- MASS::mvrnorm(n = 1, prevphi, Sigma)
    
    if(sum(propphi < 0) == 1){
      phi <- prevphi
    }else{
      u <- runif(1, 0, 1)
      # print(dphi(prevphi, y, 1, 1)/dphi(propphi, y, 1, 1))
      ifelse(u > exp(dphi(propphi, y, 1, 1)) / exp(dphi(prevphi, y, 1, 1)), phi <- prevphi, phi <- propphi)
    }
    
    # sample theta
    dlmM    <- dlm::dlmModPoly(2, dV = phi[1], dW=c(0,phi[2]))
    FiltD   <- dlm::dlmFilter(y, dlmM)
    bsample <- dlm::dlmBSample(FiltD)
    
    # save samples
    thetakeep[i,] <- c(bsample[,1],bsample[,2])
    phikeep[i,1]  <- phi[1]
    phikeep[i,2]  <- phi[2]
    prevphi       <- phi
  }
  
  # end of sampling mechanism
  return(list("theta"=thetakeep, "phi"=phikeep))
}
@

<<message=FALSE, eval=FALSE>>=
Sigma0 <- matrix(data=c(10,0,0,0.05), 2, 2, byrow=TRUE)
sigres <- mcmc2(lake, 1000, sval, Sigma0)
covmat <- cov(sigres$phi)

Sigma <- (2.4^2/2)*covmat - 0.19
sigres <- mcmc2(lake, 1000, sval, Sigma)
length(unique(sigres$phi[,1]))/1000
@

\section*{Problem 3} 
Here is the MCMC1 code and checks.

<<mcmc1, message=FALSE, cache=FALSE>>=
mcmc1 <- function(y, niter, sval){
  # Summary Stats
  n <- length(y)
  
  # setup storage
  thetakeep <- matrix(data=NA, ncol=2*(n+1), nrow=niter)
  phikeep   <- matrix(data=NA, ncol=2, nrow=niter)
  
  # setup initialization
  sigma  <- sval$v
  sigmab <- sval$w
  
  for(i in 1:niter){
    if(i%%(niter/10) == 0){print(paste(100*i/niter,"% of iterations complete", sep=""))}
    # sample theta jointly via backwards sampling
    dlmM    <- dlm::dlmModPoly(2, dV = sigma, dW=c(0,sval$w))
    FiltD   <- dlm::dlmFilter(y, dlmM)
    bsample <- dlm::dlmBSample(FiltD)
    
    # sample phi 
    SSEs   <- sum((y - bsample[-1,1])^2)
    SSEb   <- sum(diff(bsample[,2])^2)
    
    sigma  <- MCMCpack::rinvgamma(1, (n/2)+1, (SSEs/2) + 1)
    sigmab <- MCMCpack::rinvgamma(1, (n/2)+1, (SSEb/2) + 1)
    
    # save samples
    thetakeep[i,]  <- c(bsample[,1],bsample[,2])
    phikeep[i,1]   <- sigma
    phikeep[i,2]   <- sigmab
  }
  
  # end of sampling mechanism
  return(list("theta"=thetakeep, "phi"=phikeep))
}

time <- system.time(res <- mcmc1(lake, 9999, sval))
@

The effective sample size for $\sigma^2$ and $\sigma^2_{\beta}$ is $\Sexpr{mcmcse::ess(res$phi[,1])}$ and  $\Sexpr{mcmcse::ess(res$phi[,2])}$ with an average time of $\Sexpr{as.numeric(time[3]/mcmcse::ess(res$phi[,1]))}$ and  $\Sexpr{as.numeric(time[3]/mcmcse::ess(res$phi[,2]))}$ per effective sample, respectively. Histograms of the effective samples sizes for the different $\theta$ components are presented below.
<<mcmc1diag, message=FALSE, echo=FALSE, fig.align='center'>>=
efs <- data.frame(value=c(mcmcse::ess(res$theta)))
efs$variable <- rep(NA, length(efs))
efs$variable[1:87] <- "mu"
efs$variable[-c(1:87)] <- "Beta"
ggplot(efs) + geom_histogram(aes(x=value)) + facet_wrap(~variable) + theme_bw()
@


We can also see the trace plots for $\sigma^2$ and $\sigma^2_{\beta}$ that seems too out of the ordinary.
<<mcmc1traceplots, echo=FALSE, message=FALSE, fig.align='center'>>=
mphi <- melt(res$phi)
mphi$Var2 <- factor(mphi$Var2)
levels(mphi$Var2) <- c("sigma", "sigma_beta")
ggplot(mphi) + geom_line(aes(x=Var1, y=value)) + facet_wrap(~Var2, scales = "free") + 
  labs(x="Iterations", y="") + theme_bw()
@


Also for some diagnostic check, we can look at the Geweke diagnostic. For $\sigma^2$ and $\sigma^2_{\beta}$, the diagnostic is $\Sexpr{as.numeric(coda::geweke.diag(coda::mcmc(res$phi[,1]))$z)}$ and $\Sexpr{as.numeric(coda::geweke.diag(coda::mcmc(res$phi[,2]))$z)}$.
<<gewekediag, echo=FALSE, message=FALSE, fig.align='center'>>=
gd <- data.frame(value=as.numeric(coda::geweke.diag(coda::mcmc(res$theta))$z))
gd$variable <- rep(NA, length(gd))
gd$variable[1:87] <- "mu"
gd$variable[-c(1:87)] <- "Beta"
ggplot(gd) + geom_histogram(aes(x=value)) + facet_wrap(~variable) + theme_bw()
@


\section*{Problem 4} 
Here is the MCMC2 code and checks.
<<mcmc2, message=FALSE, cache=FALSE>>=
mcmc2 <- function(y, niter, sval, Sigma){
  # Summary Stats
  n <- length(y)
  
  # setup storage
  thetakeep <- matrix(data=NA, ncol=2*(n+1), nrow=niter)
  phikeep   <- matrix(data=NA, ncol=2, nrow=niter)
  
  # setup initialization
  prevphi  <- c(sval$v, sval$w)
  
  dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
  
  for(i in 1:niter){
    if(i%%(niter/10) == 0){print(paste(100*i/niter,"% of iterations complete", sep=""))}
    
    # sample phi 
    dphi <- function(phi, y, a, b){
      dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
      tmpPhi <- NULL
      dlmM      <- dlm::dlmModPoly(2, dV = phi[1], dW=c(0,phi[2]), m0=c(10,10), C0=matrix(c(10,0,0,5), 2, 2))
      FiltD     <- dlm::dlmFilter(y, dlmM)
      Rt        <- dlm::dlmSvd2var(FiltD$U.R, FiltD$D.R)
      for(t in 1:length(y)){
        prop_var <- dlmM$FF %*% Rt[[t]] %*% t(dlmM$FF) + dlmM$V
        tmpPhi[t] <- dnorm(y[t], FiltD$f[t], sqrt(prop_var), log=TRUE)
      }
      return(sum(tmpPhi) + log(dinvgamma(phi[1], a, b)) + log(dinvgamma(phi[2], a, b)))
    }
    
    propphi <- MASS::mvrnorm(n = 1, prevphi, Sigma)
    
    if(sum(propphi < 0) == 1){
      phi <- prevphi
    }else{
      u <- runif(1, 0, 1)
      # print(dphi(prevphi, y, 1, 1)/dphi(propphi, y, 1, 1))
      ifelse(u > exp(dphi(propphi, y, 1, 1)) / exp(dphi(prevphi, y, 1, 1)), phi <- prevphi, phi <- propphi)
    }
    
    # sample theta
    dlmM    <- dlm::dlmModPoly(2, dV = phi[1], dW=c(0,phi[2]))
    FiltD   <- dlm::dlmFilter(y, dlmM)
    bsample <- dlm::dlmBSample(FiltD)
    
    # save samples
    thetakeep[i,] <- c(bsample[,1],bsample[,2])
    phikeep[i,1]  <- phi[1]
    phikeep[i,2]  <- phi[2]
    prevphi       <- phi
  }
  
  # end of sampling mechanism
  return(list("theta"=thetakeep, "phi"=phikeep))
}

# using mles found above as starting values
Sigma <- matrix(data=c(9.7598368, -0.17258778, -0.1725878, 0.03841483), 2, 2, byrow=TRUE)
time2 <- system.time(res2 <- mcmc2(lake, 9999, sval, Sigma))
@

The effective sample size for $\sigma^2$ and $\sigma^2_{\beta}$ is $\Sexpr{mcmcse::ess(res2$phi[,1])}$ and  $\Sexpr{mcmcse::ess(res2$phi[,2])}$ with an average time of $\Sexpr{as.numeric(time2[3]/mcmcse::ess(res2$phi[,1]))}$ and  $\Sexpr{as.numeric(time2[3]/mcmcse::ess(res2$phi[,2]))}$ per effective sample, respectively. Histograms of the effective samples sizes for the different $\theta$ components are presented below.
<<mcmc2diag, message=FALSE, echo=FALSE, fig.align='center'>>=
efs2 <- data.frame(value=c(mcmcse::ess(res2$theta)))
efs2$variable <- rep(NA, length(efs))
efs2$variable[1:87] <- "mu"
efs2$variable[-c(1:87)] <- "Beta"
ggplot(efs2) + geom_histogram(aes(x=value)) + facet_wrap(~variable) + theme_bw()
@


We can also see the trace plots for $\sigma^2$ and $\sigma^2_{\beta}$ that seems too out of the ordinary.
<<mcmc2traceplots, echo=FALSE, message=FALSE, fig.align='center'>>=
mphi2 <- melt(res2$phi)
mphi2$Var2 <- factor(mphi2$Var2)
levels(mphi2$Var2) <- c("sigma", "sigma_beta")
ggplot(mphi2) + geom_line(aes(x=Var1, y=value)) + facet_wrap(~Var2, scales = "free") + 
  labs(x="Iterations", y="") + theme_bw()
@


Also for some diagnostic check we can look at the Geweke diagnostic. For $\sigma^2$ and $\sigma^2_{\beta}$, the diagnostic is $\Sexpr{as.numeric(coda::geweke.diag(coda::mcmc(res2$phi[,1]))$z)}$ and $\Sexpr{as.numeric(coda::geweke.diag(coda::mcmc(res2$phi[,2]))$z)}$.
<<gewekediag2, echo=FALSE, message=FALSE, fig.align='center'>>=
gd2 <- data.frame(value=as.numeric(coda::geweke.diag(coda::mcmc(res2$theta))$z))
gd2$variable <- rep(NA, length(gd2))
gd2$variable[1:87] <- "mu"
gd2$variable[-c(1:87)] <- "Beta"
ggplot(gd2) + geom_histogram(aes(x=value)) + facet_wrap(~variable) + theme_bw()
@



\section*{Problem 5.}
Posterior plots for the standard deviations to compare MCMC 1 and MCMC 2
<<message=FALSE, fig.align='center', echo=FALSE>>=
d1 <- melt(data.frame(sigma=res$phi[,1], sigma_beta=res$phi[,2]))
d1$method <- "MCMC 1"
d2 <- melt(data.frame(sigma=res2$phi[,1], sigma_beta=res2$phi[,2]))
d2$method <- "MCMC 2"
d <- rbind(d1,d2)

dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
dsqrtinvgamma = function(x, a, b) dinvgamma(x^2, a, b)*2*x

ggplot(d) + 
  geom_histogram(aes(x = sqrt(value), y = ..density..), bins = 100) + 
  facet_grid(method ~ variable, scales = "free") +
  stat_function(fun = dinvgamma, color = "red", args = list(a = 1, b = 1)) +
  theme_bw()
@

\section*{Problem 6.}
Posterior (smoothing) plots for the equal-tail 95\% credible interval for the states to compare MCMC 1 and MCMC 2.
<<message=FALSE, fig.align='center', echo=FALSE>>=
n <- length(lake)
L1 <- U1 <- L2 <- U2 <- NULL
for(i in 1:n){
  L1[i] <- quantile(res$theta[,i+1], probs=0.025)
  U1[i] <- quantile(res$theta[,i+1], probs=0.975)
  L2[i] <- quantile(res2$theta[,i+1], probs=0.025)
  U2[i] <- quantile(res2$theta[,i+1], probs=0.975)
}
dat1 <- data.frame(L=L1, U=U1)
dat1$method <- "MCMC 1"
dat1$x <- OGlake$year
dat1$ogy <- lake
dat2 <- data.frame(L=L2, U=U2)
dat2$method <- "MCMC 2"
dat2$x <- OGlake$year
dat2$ogy <- lake
d <- rbind(dat1,dat2)
d$method <- as.factor(d$method)

ggplot(d) + 
  geom_ribbon(aes(x=x, ymin=L, ymax = U, fill=method)) + 
  geom_point(aes(x=x, y=ogy)) + 
  facet_grid(method~.) +
  theme_bw() + 
  labs(x="Year", y="Precipitation")
@



Posterior (smoothing) plots for the equal-tail 95\% credible interval for the $\beta$'s to compare MCMC 1 and MCMC 2.
<<message=FALSE, fig.align='center', echo=FALSE>>=
n <- length(lake)
L1 <- U1 <- L2 <- U2 <- NULL
for(i in 1:n){
  L1[i] <- quantile(res$theta[,i+88], probs=0.025)
  U1[i] <- quantile(res$theta[,i+88], probs=0.975)
  L2[i] <- quantile(res2$theta[,i+88], probs=0.025)
  U2[i] <- quantile(res2$theta[,i+88], probs=0.975)
}
dat1 <- data.frame(L=L1, U=U1)
dat1$method <- "MCMC 1"
dat1$x <- OGlake$year
dat1$ogy <- lake
dat2 <- data.frame(L=L2, U=U2)
dat2$method <- "MCMC 2"
dat2$x <- OGlake$year
dat2$ogy <- lake
d <- rbind(dat1,dat2)
d$method <- as.factor(d$method)

ggplot(d) + 
  geom_ribbon(aes(x=x, ymin=L, ymax = U, fill=method)) + 
  facet_grid(method~.) +
  theme_bw() + 
  labs(x="Year", y="Precipitation")
@


\section*{Problem 7.}
Based on the different diagnostics and trace plots, MCMC 1 preformed better. The diagnostics look better and the implementation for MCMC 1 is much simpler. MCMC 1 is a lot quicker at getting good samples and it's Geweke diagnostics look like they come from a std normal. The posteriors of $\sigma^2_{\beta}$ don't exactly match up for the two algorithms. I think this is due to the $\Sigma$ used in MCMC 2. If I had estimated $\Sigma$ from the output of MCMC 2 then I believe I would've got more similar results, but in trying to keep with the spirit of what if MCMC 2 was the only sampler available, I got the results I got.



\end{document}