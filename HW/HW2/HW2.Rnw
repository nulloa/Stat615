\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathrsfs,fancyhdr,syntonly,lastpage,hyperref,enumitem,graphicx}

\hypersetup{colorlinks=true,urlcolor=black}

\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}


\begin{document}
\lhead{Homework 2}
\chead{STAT 615 - Advanced Bayesian Methods}
\rhead{Nehemias Ulloa}

  The Horseshoe model can be described as follows:
  \begin{align*}
  y_{ig} &\overset{ind}{\sim} N(x_i\theta_g, \sigma^2) \\
  \theta_g &\overset{ind}{\sim} N(0, \lambda_g^2 \tau^2) \\
  \lambda_g^2 &\sim Ca^{+}(0,1) \\
  \tau^2 &\sim Ca^{+}(0,1) \\
  \pi(\sigma^2) &\propto 1/\sigma^2
  \end{align*}
  
%%%%% START THE DIFF PARTS %%%%%
\begin{enumerate}
  %%%%% First Part %%%%%
  \item 
  The goal for this sampler is to make it in as few steps as possible.
  Here are the steps used:
  \begin{enumerate}
    \item \textbf{let} $i$ = 1
    %
    \item \textbf{let} $\theta_{1,0},\cdots,\theta_{n_g,0},\lambda_{1,0},\cdots,\lambda_{n_g,0}, \sigma^2_0, \tau^2_0$ be given
    %
    \item \textbf{while} $i < $ \verb|ITER|
    \begin{enumerate}
      \item \textbf{For} $g=1,\cdots,G$, sample $\theta_g \sim N(\mu_g,\gamma_g^2)$
      %
      \item \textbf{For} $g=1,\cdots,G$, sample $\lambda_g^2$ via a Metropolis-Hastings step the posterior from using the standard non-informative prior as its proposal i.e. for each $\lambda_g^2$:
      \begin{enumerate}
        \item \textbf{Draw} $\lambda_g^{2'} \sim IG\bigg(\frac{1}{2}, \frac{\theta_g^2}{2\tau^2} \bigg)$
        \item \textbf{Draw} $u$ from U(0,1)
        \item \textbf{If} $u < \frac{q(\lambda_g^{2'})}{q(\lambda_g^2)} \frac{p(\lambda_g^2)}{p(\lambda_g^{2'})}$ \textbf{then} \\
        \textbf{let} $\lambda_{g,i}^2 = \lambda_g^{2'}$
        \textbf{else} $\lambda_{g,i}^2 = \lambda_{g,i-1}^2$
      \end{enumerate}
      \end{enumerate}
      %
      \item \textbf{Sample} $\sigma^2 \sim IG\bigg(\frac{n}{2}, \frac{1}{2} \sum_{g=1}^{G} \sum_{i=1}^{n_g}(y_{ig} - \theta_g)^2\bigg)$
      \item \textbf{Sample} $\tau^2$ via a Metropolis-Hastings step the posterior from using the standard non-informative prior as its proposal i.e.
      \begin{enumerate}
        \item \textbf{Draw} $\tau^{2'} \sim IG\bigg(\frac{G}{2}, \sum_{g=1}^{G} \frac{\theta_g^2}{2 \lambda_g^2} \bigg)$
        \item \textbf{Draw} $u$ from U(0,1)
        \item \textbf{If} $u < \frac{q(\tau^{2'})}{q(\tau^2)} \frac{p(\tau^2)}{p(\tau^{2'})}$ \textbf{then} \\
        \textbf{let} $\tau_{i}^2 = \tau^{2'}$
        \textbf{else} $\tau_{i}^2 = \tau_{i-1}^2$ \\
    \end{enumerate}
  \end{enumerate}




  Full Conditionals:
  $\theta$:
  \begin{align*}
    \theta_g|\cdots &\overset{ind}{\sim} N(\mu_g,\gamma_g^2) \\
    \gamma_g^2  &= \left[ \frac{1}{\lambda_i^2 \tau_i^2} + \frac{n_g}{\sigma^2} \right]^{-1} \\
    \mu_g &= \gamma_g^2 \left[ 0*\lambda_i^{-2} \tau_i^{-2} + \bar{y}_g n_g \sigma^{-2} \right] \\ 
    &= \gamma_g^2 \left[\bar{y}_g n_g \sigma^{-2} \right] \\
  \end{align*}
  
  
  $\sigma^2$:
  \begin{equation*}
    \sigma^2|\cdots \sim IG\bigg(\frac{n}{2}, \frac{1}{2} \sum_{g=1}^{G} \sum_{i=1}^{n_g}(y_{ig} - \theta_g)^2\bigg)\\
  \end{equation*}
  
  
    $\lambda_g^2$ is sampled via a Metropolis-Hastings step since it's conditional posterior is unknown. For the proposal, we will use the posterior from using the standard non-informative prior($p(\lambda_g^2) \propto 1/\lambda_g^2$):
  \begin{align*}
    \lambda_g^2|\cdots &\sim IG\bigg(\frac{1}{2}, \frac{\theta_g^2}{2\tau^2}\bigg)\\
    q(\lambda_g^2|\cdots) &\propto \frac{1}{\sqrt{\lambda^2_g}} \frac{1}{1 + \lambda^2_g} exp\bigg( -\frac{\theta^2_g}{2\lambda^2_g \tau^2} \bigg)
  \end{align*}
  
  Similarly, $\tau^2$ is sampled via a Metropolis-Hastings step since it's conditional posterior is unknown. For the proposal, we will use the posterior from using the standard non-informative prior($p(\tau^2) \propto 1/\tau^2$):
  \begin{align*}
    \tau^2|\cdots &\sim IG\bigg(\frac{G}{2}, \sum_{g=1}^{G} \frac{\theta_g^2}{2\lambda_g^2}\bigg)\\
        q(\tau^2|\cdots) &\propto \bigg(\frac{1}{\sqrt{\tau^2}}\bigg)^G \frac{1}{1 + \tau^2} exp\bigg( -\frac{1}{2\tau^2} \sum_{g=1}^{G} \frac{\theta^2_g}{\lambda^2_g} \bigg)
  \end{align*}
  
  
  This model described above I called the Informative Horseshoe since its proposals for $\tau$ and $\lambda_g$ are fairly informative. I also ran a similar model using non-informative proposals for $\tau$ and $\lambda_g$. This will be referred to as the Vague Horseshoe.
  
  
  %%%%% Second Part %%%%%
  \item 
  Here is the samplers coded up in \verb|R|.
  <<Horseshoe_Sampler_setup, echo=TRUE, messages=FALSE>>=

  mcmc_horseshoe_inform <- function(n_reps, y, group, initial_values){
    require("invgamma")
    require("dplyr")
    
    # Setup the storage for posterior samples
    G <- length(unlist(initial_values["theta"])) # number of groups
    keep_theta  <- matrix(NA, ncol=G, nrow=n_reps)
    keep_lambda <- matrix(NA, ncol=G, nrow=n_reps)
    keep_sigma2 <- rep(NA, n_reps)
    keep_tau2   <- rep(NA, n_reps)
    
    # Set the initial values
    lambda20 <- as.numeric(unlist(initial_values["lambda"]))
    sigma20  <- as.numeric(initial_values["sigma2"])
    tau20    <- as.numeric(initial_values["tau2"])
    theta0   <- as.numeric(unlist(initial_values["theta"]))
    
    # Setup summary stats
    tmpdf <- data.frame(y, group)
    n <- length(y)
    n_g <- data.frame(tmpdf %>% group_by(group) %>% summarise(n_g = length(y)))
    n_g <- as.numeric(n_g[,2])
    ybar_g <- data.frame(tmpdf %>% group_by(group) %>% summarise(ybar_g = mean(y)))
    ybar_g <- as.numeric(ybar_g[,2])
    
    for(i in 1:n_reps){
      for(g in 1:G){
        ##### Sample Theta_g #####
        sigmaTheta <- ((1/lambda20[g]*tau20) + (n_g[g]/sigma20))^(-1)
        muTheta    <- sigmaTheta*(ybar_g[g]*n_g[g]/sigma20)
        theta      <- rnorm(1, muTheta, sigmaTheta)
        
        ###### Sample Lambda_g^2 with M-H Step #####
        al <- 1/2
        bl <- 0.5*(theta0[g]^2/tau20)
        # lambda2 <- rinvgamma(1, al, bl)
        lambda2 <- 1/rgamma(1, al, 1/bl)
        
        # Calc phi
        targetL <- function(newL, oldL, thg, t2){
          return(exp(0.5*(log(oldL) - log(newL)) + log(1+oldL^2) - log(1+newL^2) + (thg^2/(2*t2*oldL)) - (thg^2/(2*t2*newL))))
          }
        
        propL   <- function(newL, oldL, th_g, t2){
          pl <- 1.5*(log(newL) - log(oldL)) + (th_g^2/(2*t2))/newL - (th_g^2/(2*t2))/oldL
          return(exp(pl))
          }
        
        phiL <- targetL(lambda2, lambda20[g], theta0[g], tau20)#*propL(lambda2, lambda20[g], theta0[g], tau20)
        if(is.na(phiL > 0)){phiL = 0
          cat("i=",i,"g=", g,"theta=",theta0[g],"bl=",bl, "lambda=",lambda2,"lambda0=",lambda20[g],"tau20=",tau20,
                                "target=",targetL(lambda2, lambda20[g], theta0[g], tau20),"prop=",propL(lambda2, lambda20[g], theta0[g], tau20),"\n")}
        
        uL <- runif(1,0,1)
        if(uL > phiL){lambda2 <- lambda20[g]}
        
        # Update Storage
        keep_lambda[i,g]  <- lambda2
        keep_theta[i,g]   <- theta
      }
      
      
      ###### Sample Sigma^2 #####
      tmpd <- data.frame(y, group, theta = theta0[group])
      SSE <- sum((tmpd$y-tmpd$theta)^2)
      a <- n/2
      b <- 0.5*SSE
      sigma2 <- rinvgamma(1,a,b)
      
      ###### Sample Tau^2 with M-H Step #####
      at <- G/2
      bt <- 0.5*(sum(theta0^2/lambda20))
      # tau2 <- rinvgamma(1, at, bt)
      tau2 <- 1/rgamma(1, at, 1/bt)
      
      # Calc phi
      targetT <- function(newT, oldT, th, lamb, GG){
        tl <- (GG/2)*(log(oldT) - log(newT)) + log(1+oldT^2) - log(1+newT^2) - 0.5*sum(th^2/lamb)*((1/newT) + (1/oldT))
        return(exp(tl))
        }
        
      propT <- function(newT, oldT, th, lamb,GG){
        pl <- (GG/2 + 1)*(log(newT) - log(oldT)) - (0.5*sum(th^2/lamb)/oldT) + (0.5*sum(th^2/lamb)/newT)
        return(exp(pl))
        }
      
        
      phiT <- targetT(tau2, tau20, theta0, lambda20, G)#*propT(tau2, tau20, theta0, lambda20, G)
      if(is.na(phiT > 0)){phiT = 0
       print(tau2)
       cat("TauT=",targetT(tau2, tau20, theta0, lambda20, G))}
      uT <- runif(1,0,1)
      if(uT > phiT){tau2 <- tau20}
      
      # Update Storage
      keep_sigma2[i]  <- sigma2
      keep_tau2[i]    <- tau2
      
      
      # Update current values
      lambda20 <- keep_lambda[i,]
      theta0   <- keep_theta[i,]
      sigma20  <- keep_sigma2[i]
      tau20    <- keep_tau2[i]
    }
  
  
  return(list(
    theta  = keep_theta[,],
    lambda = keep_lambda[,],
    sigma2 = keep_sigma2,
    tau2   = keep_tau2
  ))
  }
  
  mcmc_horseshoe_vague <- function(n_reps, y, group, initial_values){
    require("MCMCpack")
    require("dplyr")
    
    # Setup the storage for posterior samples
    G <- length(unlist(initial_values["theta"])) # number of groups
    keep_theta  <- matrix(NA, ncol=G, nrow=n_reps)
    keep_lambda <- matrix(NA, ncol=G, nrow=n_reps)
    keep_sigma2 <- rep(NA, n_reps)
    keep_tau2   <- rep(NA, n_reps)
    
    # Set the initial values
    lambda20 <- as.numeric(unlist(initial_values["lambda"]))
    sigma20  <- as.numeric(initial_values["sigma2"])
    tau20    <- as.numeric(initial_values["tau2"])
    theta0   <- as.numeric(unlist(initial_values["theta"]))
    
    # Setup summary stats
    tmpdf <- data.frame(y, group)
    n <- length(y)
    n_g <- data.frame(tmpdf %>% group_by(group) %>% summarise(n_g = length(y)))
    n_g <- as.numeric(n_g[,2])
    ybar_g <- data.frame(tmpdf %>% group_by(group) %>% summarise(ybar_g = mean(y)))
    ybar_g <- as.numeric(ybar_g[,2])
    
    for(i in 1:n_reps){
      for(g in 1:G){
        ##### Sample Theta_g #####
        sigmaTheta <- ((1/lambda20[g]*tau20) + (n_g[g]/sigma20))^(-1)
        muTheta    <- sigmaTheta*(ybar_g[g]*n_g[g]/sigma20)
        theta      <- rnorm(1, muTheta, sigmaTheta)
        
        ###### Sample Lambda_g^2 with M-H Step #####
        al <- 1
        bl <- 2
        lambda2 <- rinvgamma(1, al, bl)
        
        # Calc phi
        targetL <- function(newL, oldL, th_g, t2){
          tl <- 0.5*(log(oldL) - log(newL)) + 2*(log(1+oldL) - log(1+newL)) + (th_g^2/(2*t2*oldL)) - (th_g^2/(2*t2*newL))
          return(exp(tl))
          }
        
        propL   <- function(newL, oldL, th_g, t2){
          pl <- (1.5)*(log(newL) - log(oldL)) - (20/oldL) + (20/newL)
          return(exp(pl))
          }
        
        phiL <- targetL(lambda2, lambda20[g], theta0[g], tau20)#*propL(lambda2, lambda20[g], theta0[g], tau20)

        uL <- runif(1,0,1)
        if(uL > phiL){lambda2 <- lambda20[g]}
        
        # Update Storage
        keep_lambda[i,g]  <- lambda2
        keep_theta[i,g]   <- theta
      }
      
      
      ###### Sample Sigma^2 #####
      tmpd <- data.frame(y, group, theta = theta0[group])
      SSE <- sum((tmpd$y-tmpd$theta)^2)
      a <- n/2
      b <- 0.5*SSE
      sigma2 <- rinvgamma(1,a,b)
      
      ###### Sample Tau^2 with M-H Step #####
      at <- 1
      bt <- 1
      tau2 <- rinvgamma(1, at, bt)
        
      # Calc phi
      targetT <- function(newT, oldT, th, lamb, GG){
        tl <- (GG/2)*(log(oldT) - log(newT)) + 2*(log(1+oldT) - log(1+newT)) - (.5*sum(th^2/lamb)/newT) + (.5*sum(th^2/lamb)/oldT)
        return(exp(tl))
        }
        
      propT <- function(newT, oldT,GG){
        pl <- (GG/2 + 1)*(log(newT) - log(oldT)) - (20/oldT) + (20/newT)
        return(exp(pl))
        }
        
      phiT <- targetT(tau2, tau20, theta0, lambda20, G)#*propT(tau2, tau20, G)
        
      uT <- runif(1,0,1)
      if(uT > phiT){tau2 <- tau20}
      
      # Update Storage
      keep_sigma2[i]  <- sigma2
      keep_tau2[i]    <- tau2
      
      
      # Update current values
      lambda20 <- keep_lambda[i,]
      theta0   <- keep_theta[i,]
      sigma20  <- keep_sigma2[i]
      tau20    <- keep_tau2[i]
    }
  
  
  return(list(
    theta  = keep_theta[,],
    lambda = keep_lambda[,],
    sigma2 = keep_sigma2,
    tau2   = keep_tau2
  ))
  }
  
  @
  
  
  
  
  %%%%% Third Part %%%%%
  \item
  For the data simulation, I set up my response and mean parameters like so:
  \begin{align*}
  \theta_{1} \cdots \theta_{20} &= 0 \\
  \theta_{21} \cdots \theta_{25} &\neq 0 \\
  y &{\sim} N(\theta, I)
  \end{align*}
  
  And in \verb|R|,
  <<data_setup, echo=TRUE, messages=FALSE, cache=TRUE>>=
  library("dplyr")
  library("Rcpp")
  sourceCpp("homework2.cpp")


  # Simulate data
set.seed(1)
G = 25
theta = c(rep(0,G-5),35,10,15,15,20)
trueTheta = theta
d = data.frame(group = rep(1:G, each=15))
d$theta = theta[d$group]
d$y = rnorm(nrow(d), d$theta)

  # Get initial values
  s <- d %>% group_by(group) %>%
    summarize(n = n(),
              mean = mean(y),
              var  = var(y))

  initial_values = list(
    lambda = s$var,
    mu = mean(s$mean), 
    theta = s$mean, 
    sigma2 = mean(s$var), 
    tau2 = var(s$mean))
  
  # Set prior
  prior = list(m = 0, C = 100, a = 1, b = 1, c = 1)
  
  # Additional initial and prior values
  initial_values$gamma = abs(s$mean) > 2*sqrt(initial_values$sigma2)
  initial_values$pi = 1-mean(initial_values$gamma)
  initial_values$psi = with(initial_values, ifelse(gamma, theta, rnorm(G, mu, sqrt(tau2))))
  initial_values$mu = 0
  initial_values$tau2 = var(s$mean[initial_values$gamma])
  if (is.na(initial_values$tau2)) initial_values$tau2 = 1
  prior$a_pi = prior$b_pi = 1
  initial_values$phi = rep(1,G)
  prior$df = 3
  
  @
  
  
  <<run_models, echo=FALSE, cache=TRUE, message=FALSE>>=
  n_reps = 10000
  InformHorseshoeRes <- mcmc_horseshoe_inform(n_reps, y = d$y, group = d$group, initial_values = initial_values)
  
  
  VagueHorseshoeRes <- mcmc_horseshoe_vague(n_reps, y = d$y, group = d$group, initial_values = initial_values)
  
  PointMassRes = mcmc_pointmass_t(n_reps, y = d$y, group = d$group, 
                          initial_values = initial_values,
                          prior = prior,
                          verbose = 0)
  @
  
  
  We ran all the models for $\Sexpr{n_reps}$ iterations. The models all ran relatively quickly even though they were coded in \verb|R|. They all preformed well in estimating the $\theta$ parameters. In Figure \ref{fig:plot_theta}, the credible intervals of the groups plotted by each model. The intervals when beta is equal to 0 are fairly tight with the point mass prior basically equaling 0 in its samples. When beta is not zero the bands are still fairly tight with the exception of the vague Horseshoe model whose bands are fairly wide due to the nature of vague proposals in it's algorithm. The informative Horseshoe however did not sample the variance parameters very well while the vague Horseshoe model did. This might be remedied if a random walk was used in the proposals for $\tau$ and $\lambda_g$ instead of the MH step.
 <<plot_theta, messgae=FALSE, echo=FALSE, fig.cap="The credible intervals of the groups plotted by each model. The intervals when beta is equal to 0 are fairly tight with the point mass prior basically equaling 0 in its samples. When beta is not zero the bands are still fairly tight with the exception of the vague Horseshoe model whose bands are fairly wide due to the nature of vague proposals in it's algorithm.">>=
  library(reshape2)
  library(ggplot2)
  # PostTheta <- data.frame(InformHorseshoeRes$theta)
  # mTheta <- melt(PostTheta)
  # names(mTheta) <- c("Theta", "Sample")
  # levels(mTheta$Theta) <- sub("X", "", levels(mTheta$Theta))
  # mTheta$trueTheta <- trueTheta[mTheta$Theta]
  # mTheta$Iter <- rep(c(1:n_reps), nlevels(mTheta$Theta))

  # ggplot(mTheta, aes(x = Sample)) + geom_histogram() + geom_vline(aes(xintercept = trueTheta), color="red") + facet_wrap(~Theta)
  # 
  # ggplot(mTheta, aes(x=Iter, y = Sample)) + geom_line() + facet_wrap(~Theta)
  # 
  # summary(PointMassRes$theta)
  # summary(InformHorseshoeRes$theta)
  
  ImTheta <- melt(data.frame(InformHorseshoeRes$theta))
  ImTheta$Model <- "InformativeHorseshoe"
  VmTheta <- melt(data.frame(VagueHorseshoeRes$theta))
  VmTheta$Model <- "VagueHorseshoe"
  PmTheta <- melt(data.frame(PointMassRes$theta))
  PmTheta$Model <- "PointMass"
  mTheta <- rbind(ImTheta,VmTheta,PmTheta)
  names(mTheta)[1:2] <- c("Group", "Sample")
  levels(mTheta$Group) <- sub("X", "", levels(mTheta$Group))
  
  CITheta <- mTheta %>%
    group_by(Group, Model) %>%
    summarise(q25 = quantile(Sample, probs=c(0.025)),
              q975 = quantile(Sample, probs=c(0.975)))
  CITheta$TrueTheta <- trueTheta[CITheta$Group]
  
  ggplot(CITheta, aes(x=q25, xend=q975, y=Group, yend=Group)) +
facet_grid(~Model) +
geom_segment() +
geom_segment(aes(x = TrueTheta, xend = TrueTheta, y=Group, yend=Group), color="red", size=10) +
# geom_vline(aes(xintercept=TrueTheta), color="red") +
#coord_cartesian(xlim=c(0,35)) +
labs(x="CI for Theta") +
theme_bw()
  
  @
  
  <<plot_lambda, eval=FALSE>>=
  Postlambda <- data.frame(InformHorseshoeRes$lambda)
  mlambda <- melt(Postlambda)
  names(mlambda) <- c("lambda", "Sample")
  levels(mlambda$lambda) <- sub("X", "", levels(mlambda$lambda))
  mlambda$Iter <- rep(c(1:n_reps), nlevels(mlambda$lambda))
  ggplot(mlambda, aes(x = Sample)) + geom_histogram() + facet_wrap(~lambda)
  ggplot(mlambda, aes(x=Iter, y = Sample)) + geom_line() + facet_wrap(~lambda)
  
  
 estl <-    mlambda %>%
    group_by(lambda) %>%
    summarise(accptprob = length(unique(Sample))/length(Sample),
              estlambda = median(Sample))
  
  
  @
  
  <<plot_sigma2_tau2, echo=FALSE, eval=FALSE>>=
  library(reshape2)
  library(ggplot2)
  
  ggplot() + geom_histogram(aes(x = InformHorseshoeRes$sigma2)) + geom_vline(xintercept = as.numeric(initial_values["sigma2"]), color="red") + scale_x_sqrt()
  ggplot() + geom_line(aes(x = 1:n_reps ,y = InformHorseshoeRes$sigma2)) + scale_y_sqrt()
  
  ggplot() + geom_histogram(aes(x = VagueHorseshoeRes$sigma2)) + geom_vline(xintercept = as.numeric(initial_values["sigma2"]), color="red") + scale_x_sqrt()
  ggplot() + geom_line(aes(x = 1:n_reps ,y = VagueHorseshoeRes$sigma2)) + scale_y_sqrt()
  
  ggplot() + geom_histogram(aes(x = PointMassRes$sigma2)) + geom_vline(xintercept = as.numeric(initial_values["sigma2"]), color="red") + scale_x_sqrt()
  ggplot() + geom_line(aes(x = 1:n_reps ,y = PointMassRes$sigma2)) + scale_y_sqrt()
  
  summary(InformHorseshoeRes$sigma2)
  
  ggplot() + geom_histogram(aes(x = InformHorseshoeRes$tau2))   + geom_vline(xintercept = as.numeric(initial_values["tau2"]), color="red") + scale_x_sqrt()
  ggplot() + geom_line(aes(x = 1:n_reps ,y = InformHorseshoeRes$tau2)) + scale_y_sqrt()
  
  ggplot() + geom_histogram(aes(x = VagueHorseshoeRes$tau2))   + geom_vline(xintercept = as.numeric(initial_values["tau2"]), color="red") + scale_x_sqrt()
  ggplot() + geom_line(aes(x = 1:n_reps ,y = VagueHorseshoeRes$tau2)) + scale_y_sqrt()
  
  ggplot() + geom_histogram(aes(x = PointMassRes$tau2))   + geom_vline(xintercept = as.numeric(initial_values["tau2"]), color="red") + scale_x_sqrt()
  ggplot() + geom_line(aes(x = 1:n_reps ,y = PointMassRes$tau2)) + scale_y_sqrt()
  
  summary(InformHorseshoeRes$tau2)
  summary(VagueHorseshoeRes$tau2)
  summary(PointMassRes$tau2)
  
  # Acceptance probability
  length(unique(InformHorseshoeRes$tau2))/length(InformHorseshoeRes$tau2)
  length(unique(VagueHorseshoeRes$tau2))/length(VagueHorseshoeRes$tau2)
  length(unique(PointMassRes$tau2))/length(PointMassRes$tau2)
  @
  
  
  
  


  

\end{enumerate}

\end{document}
